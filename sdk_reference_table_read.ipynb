{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2931f0b8-932e-4495-bcfc-b155f7926f8c",
   "metadata": {},
   "source": [
    "# SDK Reference Table - `table()` - read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa337205-736f-4da7-a3ba-4e512aae845b",
   "metadata": {},
   "source": [
    "Ocean Data Platform offers both API and Python SDK interfaces. This notebook highlights the Python SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefd9e9-e012-4e14-9c45-07074806d1e5",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f73606d-92d7-480d-abdc-8618856389d3",
   "metadata": {},
   "source": [
    "If you are not working in the [ODP Workspace](https://workspace.hubocean.earth/), you need to first install the Python SDK package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb2251-8483-4d41-995b-0037bd6d2598",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install -U odp-sdk\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e23ef82-6ba5-4e2e-a88a-2997b7b64a11",
   "metadata": {},
   "source": [
    "## Client Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83c34a6c-aa35-4588-8dd3-b9e57cfe277a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:odp.client.auth:Encryption unavailable. Opting in to plaintext\n"
     ]
    }
   ],
   "source": [
    "# Import the Client class from the odp.client module\n",
    "from odp.client import Client\n",
    "\n",
    "# Create an instance of the Client class\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc095599",
   "metadata": {},
   "source": [
    "If you are using our [ODP Workspaces](https://workspace.hubocean.earth/) you are automatically authenticated, but if you are working outside the initiation of the Client will open a browser to performance authentication process.\n",
    "If you are working outside ODP Workspaces and you don't want to open the browser to authenticate you can use API Key authentication.\n",
    "You can generate an API key in the Ocean Data Platform web interface, under your user profile.\n",
    "```python\n",
    "client = Client(api_key=\"your-api-key\")\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c338c-08f9-46f2-8270-a79e0e6787e4",
   "metadata": {},
   "source": [
    "## Dataset Access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436a77e-ce7b-4446-9603-154430148dbf",
   "metadata": {},
   "source": [
    "With an initialized `Client` you can access different datasets by using the datasets' UUID. The easiest way is to use the [ODP Catalog](https://app.hubocean.earth/catalog) to search for datasets and find the UUID (click API).\n",
    "\n",
    "For the Table examples we are using a dataset from Brazil provided from one of our partners: \n",
    "\n",
    "**Example Dataset**: PGS Biota Data - Mammal and Turtle Observations (Darwin Core Format)  \n",
    "**Dataset ID (UUID)**: `1d801817-742b-4867-82cf-5597673524eb`  \n",
    "\n",
    "**Columns in Table**:`occurrenceID`,`verbatimIdentification`,`scientificName`,`scientificNameID`,`lifeStage`,`individualCount`,`basisOfRecord`,`minimumDepthInMeters`,`eventDate`,`occurrenceRemarks`,`decimalLongitude`,`decimalLatitude`,`footprintWKT`,`license`,`occurrenceStatus`,`geodeticDatum`,`datasetName`,`institutionCode`,`otherCatalogNumbers`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9afa098-23b8-47a5-9adb-161d8a4d9cbc",
   "metadata": {},
   "source": [
    "## Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566ec9c9-3ffb-4ba4-9bd4-dacec00c72f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "dataset = client.dataset(\"1d801817-742b-4867-82cf-5597673524eb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45e693c-5241-4467-8bf0-64b457586ae2",
   "metadata": {},
   "source": [
    "The `dataset` from this UUID will be used in the examples below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e0a548-de9a-4649-9d1e-ed9d509e4076",
   "metadata": {},
   "source": [
    "## Get Dataset Schema and Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0357e-16d1-4dd2-8937-0eb22f9a85b6",
   "metadata": {},
   "source": [
    "### schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531c587-fa4a-4c19-943b-70bbc799f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table schema\n",
    "schema = dataset.table.schema()  # Returns pyarrow.Schema or None\n",
    "print(f\"Available columns: {schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec0034-e042-48b9-ab31-354a74cc0ee7",
   "metadata": {},
   "source": [
    "### stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfad0cd-493f-4d2a-9816-f86477130870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table statistics  \n",
    "stats = dataset.table.stats()\n",
    "print(f\"Total observations: {stats.num_rows:,}\")  \n",
    "print(f\"Dataset size: {stats.size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee93cf95-a262-4ca7-a746-02eaa7920824",
   "metadata": {},
   "source": [
    "## Query Table Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112f79e0-fd1b-465a-b3db-17d650936da5",
   "metadata": {},
   "source": [
    "You query the Table data by using `table.select()` followed by how you want to receive the results.\n",
    "\n",
    "There are three different ways of receiving the results from the query:\n",
    "\n",
    "Single batch\n",
    "- A GeoPandas GeoDataFrame containing all the data (for smaller datasets or a quick view of the data): `dataset.table.select().all().dataframe()`\n",
    "\n",
    "Streaming batches\n",
    "- A stream of GeoPandas GeoDataFrames (if the dataset is too large): `dataset.table.select().dataframes()`\n",
    "- A stream of PyArrow RecordBatches (a more performant way that is recommended): `dataset.table.select().batches()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc8d3ca-606a-41ae-b257-ae2a75af8af5",
   "metadata": {},
   "source": [
    "### Single batch (for smaller datasets)\n",
    "A useful way to get the data directly into a single Pandas DataFrame if you are dealing with small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffeb5a9-41de-4dd9-b516-52cb9e31fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all marine observations as single pandas DataFrame\n",
    "result_dataframe = dataset.table.select().all().dataframe()\n",
    "print(f\"Complete dataset: {len(result_dataframe)} marine observations\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a02a3c-09b9-41f9-8214-580c1bbf846d",
   "metadata": {},
   "source": [
    "You can protect from memory overflow by setting:\n",
    "* max_row (the maximum of rows to be returned in the Python DataFrame\n",
    "* max_time (time out threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5c229-ec18-4327-bfc2-f7a75149e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all marine observations as single pandas DataFrame\n",
    "result_dataframe = dataset.table.select().all(max_rows=10_000_000_000, max_time=30.0).dataframe()\n",
    "print(f\"Complete dataset: {len(result_dataframe)} marine observations\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5e8d32-8b31-443a-ad04-ba6c7cd38194",
   "metadata": {},
   "source": [
    "### Streaming batches query\n",
    "Ocean datasets are usually quite large and often it is better to get the data streaming.\n",
    "\n",
    "You have two different ways of streaming the: Pandas DataFrames (dataframes), and PyArrow RecordBatch (batches). Working with PyArrow has a performance advantage (memory efficient) and recommended if you are familiar with PyArrow (https://arrow.apache.org/docs/python/index.html), but Pandas is often what most users are more familiar with. However, it easy to convert from PyArrow RecordBatch to Pandas DataFrame.\n",
    "\n",
    "The streaming is design to allow you stop the streaming at any point when you are done with the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8f14f-7cf6-494e-918b-05b0c830bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate by Pandas DataFrames - convenient for analysis\n",
    "for dataframe_batch in dataset.table.select().dataframes():\n",
    "    print(f\"Analyzing DataFrame batch: {len(dataframe_batch)} observations\")\n",
    "    # Marine biology analysis on chunk\n",
    "    depth_stats = dataframe_batch['minimumDepthInMeters'].describe()\n",
    "    print(f\"Depth statistics: {depth_stats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a112f1eb-0d2d-4dca-b69f-acae31be9f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate by batches (PyArrow RecordBatch) - memory efficient for large datasets\n",
    "import pyarrow as pa\n",
    "\n",
    "for batch in dataset.table.select().batches():\n",
    "    print(f\"Processing batch with {batch.num_rows} observations\")\n",
    "    # Convert to Pandas\n",
    "    df_batch = batch.to_pandas()\n",
    "    # Process marine species in this batch\n",
    "    unique_species = df_batch['scientificName'].nunique()\n",
    "    print(f\"Found {unique_species} unique species in this batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c08d20d-4840-4039-b237-e9b9cdb76360",
   "metadata": {},
   "source": [
    "### Basic select() Operations\n",
    "\n",
    "Within the select() method you can pass operators to narrow down:\n",
    "* Comparison: `AND`, `OR`, `NOT` \n",
    "* Logical: `>`, `<`, `>=`, `<=`, `==`, `!=` as well as `IS NULL`,`IS NOT NULL`\n",
    "* Geospatial: `within`, `intersects`, `contains`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e4a8b9-4466-4b97-8f1d-bb918c0a3656",
   "metadata": {},
   "source": [
    "Examples are shown with single batch method, but works in the same way for streaming methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09435e-149c-49e3-95a7-1d439f8a67d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific\n",
    "dataframe = dataset.table.select(\"scientificName == 'Balaenoptera'\").all().dataframe()\n",
    "print(f\"Number of rows: {dataframe.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968a263-a10d-4f07-8245-ff3d97271f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit parameter\n",
    "dataframe = dataset.table.select(filter=\"scientificName == 'Balaenoptera'\").all().dataframe()\n",
    "print(f\"Number of rows: {dataframe.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff3c1ce-c06b-49c8-ae04-8d261c90e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns which is more efficient than selecting all columns and filtering in Python\n",
    "dataframe = dataset.table.select(\n",
    "    \"minimumDepthInMeters > 100\", \n",
    "    cols=[\"scientificName\", \"lifeStage\", \"minimumDepthInMeters\", \"eventDate\"]\n",
    ").all().dataframe()\n",
    "print(f\"Number of rows: {dataframe.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87732bed-448c-41bf-8ee6-01cc0db72825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select with multiple variables\n",
    "dataframe = dataset.select(\n",
    "    \"scientificName == 'Balaenoptera' AND minimumDepthInMeters > 100\"\n",
    ").all().dataframe()\n",
    "print(f\"Number of rows: {dataframe.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a7d54-d675-48bf-a68f-f1d8333ff46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select with named bind variables for safe, efficient queries:\n",
    "dataframe = dataset.table.select(\n",
    "    \"scientificName == $species\",\n",
    "    vars={\n",
    "        \"species\": \"Balaenoptera\"\n",
    "    }\n",
    ").all().dataframe()\n",
    "print(f\"Number of rows: {dataframe.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa167c-c969-4793-9765-bd7d04602706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to work with geo\n",
    "dataframe = dataset.table.select(\n",
    "    'footprintWKT within $area', \n",
    "    vars={\"area\": \"POLYGON((-37 -12, -45 -26, -40 -28, -33 -13, -37 -12))\"},\n",
    ").all().dataframe()\n",
    "print(f\"Number of rows: {dataframe.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd466a5-5457-40b5-834d-ab33850a7266",
   "metadata": {},
   "source": [
    "### Aggregations\n",
    "Some description on aggregations\n",
    "- max\n",
    "- min\n",
    "- sum\n",
    "- count\n",
    "- mean (average)\n",
    "\n",
    "Geo aggregations\n",
    "- h3 (read more about it here: https://h3geo.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78f23f8-5f2e-4adf-a7a1-996e2cd99199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by a column\n",
    "dataframe = dataset.table.aggregate(\n",
    "    group_by=\"lifeStage\",\n",
    "    aggr={\"minimumDepthInMeters\": \"mean\"}\n",
    ")\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a155cdd7-d5da-40c2-af68-25dba445b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by a column combined with a query\n",
    "dataframe = dataset.table.aggregate(\n",
    "    group_by=\"scientificName\",\n",
    "    filter=\"scientificName IS NOT NULL AND minimumDepthInMeters IS NOT NULL\",\n",
    "    aggr={\n",
    "        \"minimumDepthInMeters\": \"mean\"\n",
    "    }\n",
    ")\n",
    "print(dataframe.iloc[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57697da4-725a-4503-9c91-7df14e78bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate without grouping\n",
    "dataframe = dataset.table.aggregate(\n",
    "    group_by='\"TOTAL\"',  # Special value\n",
    "    aggr={ \n",
    "        \"minimumDepthInMeters\": \"max\"\n",
    "    }\n",
    ")\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef36eb-8179-4d6e-8798-56d95979857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by h3 hexagons\n",
    "dataframe = dataset.table.aggregate(\n",
    "    group_by=\"h3(footprintWKT, 5)\", # Arguments: Column containing the geometry, and resolution between 0 and 15 https://h3geo.org/docs/core-library/restable/\n",
    "    filter=\"footprintWKT IS NOT NULL\",\n",
    "    aggr={\n",
    "        \"minimumDepthInMeters\": \"mean\"\n",
    "    }\n",
    ")\n",
    "print(dataframe.iloc[0:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d5f14e-4206-45f2-be80-ba1f09add415",
   "metadata": {},
   "source": [
    "### Performance tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6842037-8819-4778-b341-163bcbeab8a7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Use column selection: Only select columns you need.\n",
    "2. Use bind variables: More efficient than string concatenation (safer as well).\n",
    "3. Filter early: Apply filters in the query rather than in Python.\n",
    "4. Consider aggregation: Use aggregate() instead of selecting all data and aggregating in Python.\n",
    "5. Use Streaming: Handle large datasets by streaming and iterate them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fa13c9-eec9-4cd9-9cdb-617ca9cf020d",
   "metadata": {},
   "source": [
    "### Error handling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b506e103-ce51-442a-bfd5-0413176a576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "try:\n",
    "    result = dataset.table.select(\"invalid_column =! 5\").all().dataframe()\n",
    "except ValueError as e:\n",
    "    print(f\"Query error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
